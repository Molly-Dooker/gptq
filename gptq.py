import math
import time

import torch
import torch.nn as nn
import transformers

from quant import *


DEBUG = False

torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False


class GPTQ:

    def __init__(self, layer):
        self.layer = layer # ì–‘ìí™”ë¥¼ ìˆ˜í–‰í•  ë ˆì´ì–´
        self.dev = self.layer.weight.device # ë ˆì´ì–´ ê°€ì¤‘ì¹˜ì˜ ë””ë°”ì´ìŠ¤ (CPU ë˜ëŠ” CUDA)
        W = layer.weight.data.clone() # ë ˆì´ì–´ ê°€ì¤‘ì¹˜ ë³µì‚¬
        if isinstance(self.layer, nn.Conv2d): # 2D í•©ì„±ê³± ë ˆì´ì–´ì¸ ê²½ìš°
            W = W.flatten(1) # ê°€ì¤‘ì¹˜ë¥¼ 2Dë¡œ í¼ì¹¨
        if isinstance(self.layer, transformers.Conv1D): # 1D í•©ì„±ê³± ë ˆì´ì–´ì¸ ê²½ìš° (transformers ë¼ì´ë¸ŒëŸ¬ë¦¬)
            W = W.t() # ê°€ì¤‘ì¹˜ ì „ì¹˜
        self.rows = W.shape[0] # ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ í–‰ ìˆ˜
        self.columns = W.shape[1] # ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ ì—´ ìˆ˜
        self.H = torch.zeros((self.columns, self.columns), device=self.dev) # í—¤ì‹œì•ˆ í–‰ë ¬ H ì´ˆê¸°í™” (ì—´ x ì—´ í¬ê¸°)
        self.nsamples = 0 # ë³´ì • ë°ì´í„° ìƒ˜í”Œ ìˆ˜ ì´ˆê¸°í™”

    def add_batch(self, inp, out): # pylint: disable=unused-argument
        # ì´ ë©”ì†Œë“œëŠ” í—¤ì‹œì•ˆ í–‰ë ¬ Hë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì…ë ¥(inp) ë°°ì¹˜ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        # ë…¼ë¬¸ì—ì„œëŠ” H = 2 * X * X^T ë¡œ ì •ì˜ë˜ë©°, ì—¬ê¸°ì„œ XëŠ” ì…ë ¥ ë°ì´í„°ì…ë‹ˆë‹¤. [cite: 759]
        # ì½”ë“œì—ì„œëŠ” Hë¥¼ ì ì§„ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ë©° í‰ê· ì„ ëƒ…ë‹ˆë‹¤.
        if DEBUG:
            self.inp1 = inp
            self.out1 = out
        if len(inp.shape) == 2: # ì…ë ¥ì´ 2D í…ì„œì¸ ê²½ìš° (ë³´í†µ ë°°ì¹˜ í¬ê¸° x íŠ¹ì„± ìˆ˜)
            inp = inp.unsqueeze(0) # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (1 x ë°°ì¹˜ í¬ê¸° x íŠ¹ì„± ìˆ˜)
        tmp = inp.shape[0] # í˜„ì¬ ë°°ì¹˜ì˜ ìƒ˜í”Œ ìˆ˜
        if isinstance(self.layer, nn.Linear) or isinstance(self.layer, transformers.Conv1D):
            # ì„ í˜• ê³„ì¸µ ë˜ëŠ” Conv1D ê³„ì¸µì˜ ê²½ìš°
            if len(inp.shape) == 3: # ì…ë ¥ì´ 3D í…ì„œì¸ ê²½ìš° (ìƒ˜í”Œ ìˆ˜ x ì‹œí€€ìŠ¤ ê¸¸ì´ x íŠ¹ì„± ìˆ˜)
                inp = inp.reshape((-1, inp.shape[-1])) # 2Dë¡œ ë³€í˜• (ìƒ˜í”Œ ìˆ˜ * ì‹œí€€ìŠ¤ ê¸¸ì´ x íŠ¹ì„± ìˆ˜)
            inp = inp.t() # (íŠ¹ì„± ìˆ˜ x ìƒ˜í”Œ ìˆ˜ * ì‹œí€€ìŠ¤ ê¸¸ì´)ë¡œ ì „ì¹˜
        if isinstance(self.layer, nn.Conv2d): # 2D í•©ì„±ê³± ê³„ì¸µì˜ ê²½ìš°
            # ì…ë ¥ ë°ì´í„°ë¥¼ im2colê³¼ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ í¼ì³ì„œ ì„ í˜• ê³„ì¸µì²˜ëŸ¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•¨
            unfold = nn.Unfold(
                self.layer.kernel_size,
                dilation=self.layer.dilation,
                padding=self.layer.padding,
                stride=self.layer.stride
            )
            inp = unfold(inp) # (ë°°ì¹˜ í¬ê¸° x (ì±„ë„ ìˆ˜ * ì»¤ë„ ë†’ì´ * ì»¤ë„ ë„ˆë¹„) x L) ì—¬ê¸°ì„œ Lì€ ê²°ê³¼ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ì˜ ìˆ˜
            inp = inp.permute([1, 0, 2]) # ((ì±„ë„ ìˆ˜ * ì»¤ë„ ë†’ì´ * ì»¤ë„ ë„ˆë¹„) x ë°°ì¹˜ í¬ê¸° x L)
            inp = inp.flatten(1) # ((ì±„ë„ ìˆ˜ * ì»¤ë„ ë†’ì´ * ì»¤ë„ ë„ˆë¹„) x (ë°°ì¹˜ í¬ê¸° * L))
        self.H *= self.nsamples / (self.nsamples + tmp) # ê¸°ì¡´ Hê°’ì— ì´ì „ ìƒ˜í”Œ ìˆ˜ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ ì ìš© (í‰ê·  ì—…ë°ì´íŠ¸)
        self.nsamples += tmp # ì´ ìƒ˜í”Œ ìˆ˜ ì—…ë°ì´íŠ¸
        # inp = inp.float()
        inp = math.sqrt(2 / self.nsamples) * inp.float() # ì…ë ¥ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (ë…¼ë¬¸ì˜ H = 2XX^T ì— ë§ì¶”ê¸° ìœ„í•¨) [cite: 759]
        # self.H += 2 / self.nsamples * inp.matmul(inp.t()) # Hì— í˜„ì¬ ë°°ì¹˜ ê¸°ì—¬ë¶„ ì¶”ê°€ (ì›ë˜ OBS ê³µì‹ì— ë” ê°€ê¹Œì›€)
        self.H += inp.matmul(inp.t()) # ìŠ¤ì¼€ì¼ë§ëœ ì…ë ¥ì„ ì‚¬ìš©í•˜ì—¬ H ì—…ë°ì´íŠ¸

    def fasterquant(
        self, blocksize=128, percdamp=.01, groupsize=-1, actorder=False, static_groups=False
    ):
        # ì´ ë©”ì†Œë“œê°€ ë…¼ë¬¸ì˜ Algorithm 1ì— í•´ë‹¹í•˜ëŠ” í•µì‹¬ ì–‘ìí™” ë¡œì§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. [cite: 802]
        W = self.layer.weight.data.clone() # ì–‘ìí™”í•  ê°€ì¤‘ì¹˜ W ë³µì‚¬
        if isinstance(self.layer, nn.Conv2d): # 2D í•©ì„±ê³± ë ˆì´ì–´ ì²˜ë¦¬
            W = W.flatten(1)
        if isinstance(self.layer, transformers.Conv1D): # 1D í•©ì„±ê³± ë ˆì´ì–´ ì²˜ë¦¬ (transformers)
            W = W.t()
        W = W.float() # ê°€ì¤‘ì¹˜ë¥¼ float32ë¡œ ë³€í™˜í•˜ì—¬ ì •ë°€ë„ ë†’ì€ ê³„ì‚° ìˆ˜í–‰

        tick = time.time() # ì‹œê°„ ì¸¡ì • ì‹œì‘

        if not self.quantizer.ready(): # ì–‘ìí™”ê¸°(Quantizer)ê°€ ì•„ì§ ì„¤ì •ë˜ì§€ ì•Šì•˜ë‹¤ë©´
            self.quantizer.find_params(W, weight=True) # W ì „ì²´ì— ëŒ€í•´ ì–‘ìí™” íŒŒë¼ë¯¸í„°(scale, zero-point) ê³„ì‚° [cite: 754]

        # HëŠ” add_batchë¥¼ í†µí•´ ëˆ„ì ëœ í—¤ì‹œì•ˆ ê·¼ì‚¬ í–‰ë ¬ì…ë‹ˆë‹¤.
        H = self.H # add_batchì—ì„œ ê³„ì‚°ëœ í—¤ì‹œì•ˆ í–‰ë ¬
        del self.H # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì›ë³¸ H ì‚­ì œ
        dead = torch.diag(H) == 0 # í—¤ì‹œì•ˆ ëŒ€ê° ì„±ë¶„ì´ 0ì¸ ì—´ (ê±°ì˜ ì—…ë°ì´íŠ¸ë˜ì§€ ì•ŠëŠ” ê°€ì¤‘ì¹˜) ì‹ë³„
        H[dead, dead] = 1 # í•´ë‹¹ ì—´ì˜ í—¤ì‹œì•ˆ ê°’ì„ 1ë¡œ ì„¤ì •í•˜ì—¬ ìˆ˜ì¹˜ì  ì•ˆì •ì„± í™•ë³´ (ì—­í–‰ë ¬ ê³„ì‚° ì‹œ ë¬¸ì œ ë°©ì§€)
        W[:, dead] = 0 # í•´ë‹¹ ì—´ì˜ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì„¤ì • (ì˜í–¥ ì—†ëŠ” ê°€ì¤‘ì¹˜)

        # ì •ì  ê·¸ë£¹ (static_groups) ì‚¬ìš© ì‹œ, ê·¸ë£¹ë³„ë¡œ ì–‘ìí™” íŒŒë¼ë¯¸í„°ë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•©ë‹ˆë‹¤.
        # ë…¼ë¬¸ì—ì„œëŠ” ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ì§€ë§Œ, ê·¸ë£¹ ì–‘ìí™”ì˜ í•œ ë³€í˜•ì…ë‹ˆë‹¤. [cite: 884]
        if static_groups:
            import copy
            groups = []
            for i in range(0, self.columns, groupsize): # groupsize ë‹¨ìœ„ë¡œ ì—´ ë¶„í• 
                quantizer = copy.deepcopy(self.quantizer) # ê¸°ë³¸ ì–‘ìí™”ê¸° ë³µì‚¬
                quantizer.find_params(W[:, i:(i + groupsize)], weight=True) # ê° ê·¸ë£¹ì— ëŒ€í•´ ì–‘ìí™” íŒŒë¼ë¯¸í„° ê³„ì‚°
                groups.append(quantizer)

        # í™œì„±í™” ìˆœì„œ (actorder) ì ìš©: í—¤ì‹œì•ˆ ëŒ€ê° ì„±ë¶„(ì¤‘ìš”ë„ ì¶”ì •ì¹˜) ê¸°ì¤€ìœ¼ë¡œ ì—´ ì¬ì •ë ¬
        # ë…¼ë¬¸ì—ì„œëŠ” ì–¸ê¸‰ë˜ì§€ ì•Šì€ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ, LLaMa ëª¨ë¸ì—ì„œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì„
        if actorder:
            perm = torch.argsort(torch.diag(H), descending=True) # í—¤ì‹œì•ˆ ëŒ€ê°ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ ì¸ë±ìŠ¤
            W = W[:, perm] # ê°€ì¤‘ì¹˜ ì—´ ì¬ì •ë ¬
            H = H[perm][:, perm] # í—¤ì‹œì•ˆ í–‰/ì—´ ì¬ì •ë ¬
            invperm = torch.argsort(perm) # ì›ë˜ ìˆœì„œë¡œ ë˜ëŒë¦¬ê¸° ìœ„í•œ ì—­ìˆœì—´

        Losses = torch.zeros_like(W) # ì–‘ìí™”ë¡œ ì¸í•œ ì†ì‹¤ ì €ì¥í•  í…ì„œ
        Q = torch.zeros_like(W) # ì–‘ìí™”ëœ ê°€ì¤‘ì¹˜ ì €ì¥í•  í…ì„œ. Algorithm 1ì˜ Qì™€ ë™ì¼ [cite: 802]

        # í—¤ì‹œì•ˆ í–‰ë ¬ ì •ê·œí™” (Dampening) ë° ì½œë ˆìŠ¤í‚¤ ë¶„í•´ë¥¼ ì´ìš©í•œ ì—­í–‰ë ¬ ê³„ì‚° ì¤€ë¹„
        # ë…¼ë¬¸ Step 3: Cholesky Reformulationì— í•´ë‹¹ [cite: 792, 796, 797]
        # Algorithm 1ì˜ H^-1 <- (2XX^T + lambda*I)^-1 ì— í•´ë‹¹í•˜ë©°, lambda*Iê°€ dampening ì—­í•  [cite: 802]
        damp = percdamp * torch.mean(torch.diag(H)) # í‰ê·  ëŒ€ê°ê°’ì˜ ì¼ì • ë¹„ìœ¨ë¡œ ê°ì‡ ê°’ ì„¤ì • [cite: 792]
        diag = torch.arange(self.columns, device=self.dev) # ëŒ€ê°ì„  ì¸ë±ìŠ¤
        H[diag, diag] += damp # í—¤ì‹œì•ˆ ëŒ€ê°ì„ ì— ê°ì‡ ê°’ ì¶”ê°€ (ìˆ˜ì¹˜ ì•ˆì •ì„± ë° ì •ì¹™í™” íš¨ê³¼)
        H = torch.linalg.cholesky(H) # ì½œë ˆìŠ¤í‚¤ ë¶„í•´ L (H = L * L^T)
        H = torch.cholesky_inverse(H) # ì½œë ˆìŠ¤í‚¤ ë¶„í•´ë¥¼ ì´ìš©í•˜ì—¬ Hì˜ ì—­í–‰ë ¬ ê³„ì‚° (H^-1)
        H = torch.linalg.cholesky(H, upper=True) # H^-1ì˜ ì½œë ˆìŠ¤í‚¤ ë¶„í•´ U (H^-1 = U^T * U), ì—¬ê¸°ì„œ Uê°€ Hinv. Algorithm 1ì˜ H^-1 <- Cholesky(H^-1)^T ì— í•´ë‹¹ [cite: 802]
        Hinv = H # HinvëŠ” H^-1ì˜ ì½œë ˆìŠ¤í‚¤ ë¶„í•´ì˜ ìƒì‚¼ê° í–‰ë ¬ Uë¥¼ ì˜ë¯¸

        # ì—´ì„ ë¸”ë¡ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (Lazy Batch-Updates)
        # ë…¼ë¬¸ Step 2: Lazy Batch-Updates ì— í•´ë‹¹í•˜ë©°, B=128ì„ ì‚¬ìš© [cite: 785]
        # Algorithm 1ì˜ ë°”ê¹¥ìª½ ë£¨í”„ (for i = 0, B, 2B, ... d_col) [cite: 802]
        for i1 in range(0, self.columns, blocksize): # blocksize ë‹¨ìœ„ë¡œ ì—´ ì²˜ë¦¬
            i2 = min(i1 + blocksize, self.columns) # í˜„ì¬ ë¸”ë¡ì˜ ë ì¸ë±ìŠ¤
            count = i2 - i1 # í˜„ì¬ ë¸”ë¡ì˜ ì—´ ê°œìˆ˜

            W1 = W[:, i1:i2].clone() # í˜„ì¬ ë¸”ë¡ì˜ ê°€ì¤‘ì¹˜ ë³µì‚¬
            Q1 = torch.zeros_like(W1) # í˜„ì¬ ë¸”ë¡ì˜ ì–‘ìí™”ëœ ê°€ì¤‘ì¹˜
            Err1 = torch.zeros_like(W1) # í˜„ì¬ ë¸”ë¡ì˜ ì–‘ìí™” ì˜¤ì°¨ (ìŠ¤ì¼€ì¼ë§ë¨)
            Losses1 = torch.zeros_like(W1) # í˜„ì¬ ë¸”ë¡ì˜ ì†ì‹¤
            Hinv1 = Hinv[i1:i2, i1:i2] # í˜„ì¬ ë¸”ë¡ì— í•´ë‹¹í•˜ëŠ” Hinvì˜ ë¶€ë¶„ í–‰ë ¬ (B x B í¬ê¸°)

            # ë¸”ë¡ ë‚´ ê° ì—´ì„ ìˆœì°¨ì ìœ¼ë¡œ ì–‘ìí™”
            # Algorithm 1ì˜ ì•ˆìª½ ë£¨í”„ (for j = i, ..., i+B-1) [cite: 802]
            for i in range(count): # í˜„ì¬ ë¸”ë¡ ë‚´ ê° ì—´ì— ëŒ€í•´ ë°˜ë³µ
                w = W1[:, i] # í˜„ì¬ ì–‘ìí™”í•  ì—´ (ë²¡í„°)
                d = Hinv1[i, i] # Hinv1ì˜ ëŒ€ê° ì„±ë¶„. ë…¼ë¬¸ Algorithm 1ì˜ [H^-1]_jj ì— í•´ë‹¹ [cite: 802]

                # ê·¸ë£¹ ì–‘ìí™” ì²˜ë¦¬
                if groupsize != -1: # ê·¸ë£¹ í¬ê¸°ê°€ ì§€ì •ëœ ê²½ìš°
                    if not static_groups: # ë™ì  ê·¸ë£¹: groupsize ë§ˆë‹¤ ì–‘ìí™” íŒŒë¼ë¯¸í„° ì¬ê³„ì‚°
                        if (i1 + i) % groupsize == 0:
                            self.quantizer.find_params(W[:, (i1 + i):(i1 + i + groupsize)], weight=True) [cite: 861]
                    else: # ì •ì  ê·¸ë£¹: ë¯¸ë¦¬ ê³„ì‚°ëœ ê·¸ë£¹ë³„ ì–‘ìí™”ê¸° ì‚¬ìš©
                        idx = i1 + i
                        if actorder: # actorder ì‚¬ìš© ì‹œ ì›ë˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
                            idx = perm[idx].item() # .item() ì¶”ê°€í•˜ì—¬ ìŠ¤ì¹¼ë¼ ê°’ìœ¼ë¡œ ë³€í™˜
                        self.quantizer = groups[idx // groupsize]

                # ê°€ì¤‘ì¹˜ ì–‘ìí™”: wë¥¼ quantizerì˜ scale, zero-pointë¥¼ ì‚¬ìš©í•´ ì–‘ìí™” ê·¸ë¦¬ë“œì˜ ê°€ì¥ ê°€ê¹Œìš´ ê°’ìœ¼ë¡œ ë°˜ì˜¬ë¦¼
                # ë…¼ë¬¸ Algorithm 1ì˜ Q_:,j <- quant(W_:,j) ì— í•´ë‹¹ [cite: 802]
                q = quantize(
                    w.unsqueeze(1), self.quantizer.scale, self.quantizer.zero, self.quantizer.maxq
                ).flatten()
                Q1[:, i] = q # ì–‘ìí™”ëœ ê°€ì¤‘ì¹˜ ì €ì¥
                Losses1[:, i] = (w - q) ** 2 / d ** 2 # ì–‘ìí™” ì˜¤ì°¨ë¡œ ì¸í•œ ì†ì‹¤ ê³„ì‚° (OBQ ìˆ˜ì‹ (2)ì˜ ë¶„ì ë¶€ë¶„ê³¼ ìœ ì‚¬) [cite: 759]

                # ì–‘ìí™” ì˜¤ì°¨ ê³„ì‚° ë° ë‚˜ë¨¸ì§€ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (ë¸”ë¡ ë‚´)
                # ë…¼ë¬¸ Algorithm 1ì˜ E_:,j-i <- (W_:,j - Q_:,j) / [H^-1]_jj ì™€ W_:,j:(i+B) <- W_:,j:(i+B) - E_:,j-i * H_j,j:(i+B)^-1 ì— í•´ë‹¹ [cite: 802]
                err1 = (w - q) / d # ìŠ¤ì¼€ì¼ë§ëœ ì–‘ìí™” ì˜¤ì°¨. Algorithm 1ì˜ E_:,j-i [cite: 802]
                W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0)) # ë¸”ë¡ ë‚´ ë‚˜ë¨¸ì§€ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (OBS ìŠ¤íƒ€ì¼) [cite: 759]
                Err1[:, i] = err1 # ìŠ¤ì¼€ì¼ë§ëœ ì˜¤ì°¨ ì €ì¥

            Q[:, i1:i2] = Q1 # ì „ì²´ Q í–‰ë ¬ì— í˜„ì¬ ë¸”ë¡ì˜ ì–‘ìí™”ëœ ê°€ì¤‘ì¹˜ ì €ì¥
            Losses[:, i1:i2] = Losses1 / 2 # ì†ì‹¤ ì €ì¥ (2ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì€ OBS ê³µì‹ê³¼ ê´€ë ¨ë  ìˆ˜ ìˆìŒ)

            # ë¸”ë¡ ì™¸ë¶€ì˜ ë‚˜ë¨¸ì§€ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (ì „ì—­ ì—…ë°ì´íŠ¸)
            # ë…¼ë¬¸ Algorithm 1ì˜ W_:, (i+B): <- W_:, (i+B): - E * H_i:(i+B), (i+B):^-1 ì— í•´ë‹¹ [cite: 802]
            # ë…¼ë¬¸ ìˆ˜ì‹ (4)ì™€ ê´€ë ¨ë¨: ğ›¿_F = -(w_Q - quant(w_Q)) * ([H_F^-1]_QQ)^-1 * (H_F^-1)_:,Q [cite: 786]
            # ì—¬ê¸°ì„œ Err1ì´ (w_Q - quant(w_Q)) / d ì— í•´ë‹¹í•˜ê³ , Hinvê°€ H_F^-1ì˜ ì½œë ˆìŠ¤í‚¤ ì¸ìì´ë¯€ë¡œ,
            # Err1.matmul(Hinv[i1:i2, i2:])ëŠ” ë‚˜ë¨¸ì§€ ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ë³´ìƒ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])

            if DEBUG: # ë””ë²„ê¹… ëª©ì ìœ¼ë¡œ ì¤‘ê°„ ê²°ê³¼ í™•ì¸
                self.layer.weight.data[:, :i2] = Q[:, :i2]
                self.layer.weight.data[:, i2:] = W[:, i2:]
                print(torch.sum((self.layer(self.inp1) - self.out1) ** 2))
                print(torch.sum(Losses))

        torch.cuda.synchronize() # CUDA ì—°ì‚° ì™„ë£Œ ëŒ€ê¸°
        print('time %.2f' % (time.time() - tick)) # ì´ ì–‘ìí™” ì‹œê°„ ì¶œë ¥
        print('error', torch.sum(Losses).item()) # ìµœì¢… ì–‘ìí™” ì˜¤ì°¨ í•©ê³„ ì¶œë ¥

        if actorder: # actorderê°€ ì ìš©ëœ ê²½ìš°, Që¥¼ ì›ë˜ ìˆœì„œë¡œ ë³µì›
            Q = Q[:, invperm]

        if isinstance(self.layer, transformers.Conv1D): # Conv1Dì˜ ê²½ìš° ë‹¤ì‹œ ì „ì¹˜
            Q = Q.t()
        # ìµœì¢… ì–‘ìí™”ëœ ê°€ì¤‘ì¹˜ Që¥¼ ì›ë˜ ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ë¡œ í• ë‹¹
        self.layer.weight.data = Q.reshape(self.layer.weight.shape).to(self.layer.weight.data.dtype)
        if DEBUG:
            print(torch.sum((self.layer(self.inp1) - self.out1) ** 2))

    def free(self):
        # ë©”ëª¨ë¦¬ í•´ì œ
        if DEBUG:
            self.inp1 = None
            self.out1 = None
        # self.HëŠ” fasterquant ì‹œì‘ ì‹œ del Hë¡œ ì´ë¯¸ í•´ì œë¨
        # self.Losses
        # self.Trace
        torch.cuda.empty_cache()